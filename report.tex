\documentclass[titlepage, twocolumn]{article}

\usepackage{graphicx}
\usepackage{tabularx}
\graphicspath{{./figures}}

\title{%
    Engineering Physics 353 Final Report \\
    \large Team 14 - ``Team 4"}
\author{Nathan Van Rumpt, Eric Souder}
\date{April 2023}

\begin{document}
\maketitle

\section{Background}

    The final project for Engineering Physics 353 is the creation of ROS packages to control a simulated robot as it attempts to drive around a streetscape - composed of an inner and outer loop - and record license plate numbers off of parked cars [figure?]. The robot must be capable of navigating entirely autonomously while avoiding collisions with pedestrians and other vehicles in the environment. Points were awarded for correctly reporting a license plate over a ROS topic (6 points for plates on the outer ring, and 8 points for plates on the inner ring) and for completing a full lap, and deducted for hitting pedestrians, other cars, or driving off the road. Our team attempted to create a robot that would be primarily capable of very reliably navigating the track and collecting the maximum amount of points. As a secondary objective, we aimed to have the robot complete as quickly as possible, as tie scores would be broken by the robot with the fastest time.

\section{Overall Architecture}

    The robot control software was divided into two ROS packages - one which managed the navigation of the robot and guided it around the track, and one which managed the license plate detection and reporting. The navigation package was composed of a state machine node and a navigation node, which were responsible for controlling the robot's movement around the track, while the plate detection packaged contained a single node responsible for detecting license plates and reporting them over a ROS topic. Additionally, a graphical user interface used for debugging and running the robot was developed and stored in a separate repository. The overall architecture of the robot is shown in Figure~\ref{fig:architecture}. 

    \begin{figure}
        \includegraphics[width=\linewidth]{architecture.png}
        \caption{Overall architecture of the robot control software.}
        \label{fig:architecture}
    \end{figure}

\section{Navigation}
    Robot navigation was accomplished with an almost entirely classical control system, using minimal neural network-based approaches and instead using manually-tuned computer vision and classical control techniques. 

    The navigation controller was composed of two ROS nodes: The `State Machine' node was composed of a finite state machine and the `Navigate' node contained a set of navigation algorithms for different scenarios, selecting an appropriate algorithm based on the state reported by the state machine. States were communicated internally between the two nodes via a ROS topic.
    
    \subsection{State Machine}
        The finite state machine responsible for controlling the robot's navigation state was composed primarily of a number of state classes, representing a unique navigational state and a state machine class responsible for orchestrating the transitions between them. Each state class, such as the `Pave Navigate' or `Junction Wait' states, is a subclass of an abstract parent class which defines an `evaluate transition' interface responsible for determining the next state to enter and either returns itself to remain in the current state or generates a new state object to transition into. The state machine class calls this method each frame to determine which state the robot should be in, and then broadcasts that state to the navigation node via a ROS topic. The overall state machine diagram is shown in Figure~\ref{fig:statemachine}.

        \begin{figure}
            \includegraphics[width=\linewidth]{statemachine.png}
            \caption{Diagram of the robot state machine.}
            \label{fig:statemachine}
        \end{figure}

        Three main types of navigational states existed: active navigation states, stopped states, and preprogrammed states. In preprogrammed states, where the robot executes a constant movement, the state machine transitions based off of a simple timer to move to the next state once the movement had been in progress for the preprogrammed length of time. In active navigation states, where the navigation algorithm is interpreting each frame on the fly to determine a which movement to command, or in stopped states, where the robot is waiting for an event to occur before proceeding, the state machine interpets each frame and makes a transition decision based on it.

        \subsubsection{Pavement and Grass Detection}
            In order to transition from the pavement navigation states to the grass navigation state and vice versa, the state machine must be able to determine if the robot is driving on grass or on pavement. This detection is accomplished with a small neural network, implemeted within a `SurfaceDetector' class. Training data was 400 images captured from the robot camera, evenly split between grass and pavement surfaces. Data was aqquired by by manually driving the vehicle around with a simple ros node activated to save each camera frame. Because of the goal of simplicity for the neural network, all images were preprocessed by signifigant compression, images were resized to 0.025 times their original width and 0.05 times their original height, this resulted in a 36 by 32 image, which was further cropped to remove the top 18 rows of pixels, focusing only on the ground and removing the area above the horizon.

            The simple model was composed of only three layers: An input flatten layer, a hidden dense layer, and a output dense layer composed of a single neuron, with a total of 2,666,689 trainable parameters, as shown in Figure~\ref{fig:surfacedetectmodel}.
            \begin{figure}
                \begin{tabularx}{0.9\linewidth}{ 
                     >{\raggedright\arraybackslash}X 
                     >{\raggedright\arraybackslash}X 
                     >{\raggedleft\arraybackslash}X  }

                     Layer (type) & Output Shape & Num. Params. \\ 
                    \hline \\
                    Flatten & (None, 1632) & 0 \\  
                    Dense & (None, 1632) & 2665056 \\
                    Dense & (None, 1) & 1633 \\
                \end{tabularx}
                \caption{Summary of surface detection neural network}
                \label{fig:surfacedetectmodel}
            \end{figure}
            The model was trained with a learning rate of $1 \times 10^{-4}$ and a validation split of 0.2 over 10 epochs. Loss and accuracy curves by epoch are shown in Figure~\ref{fig:grasspavegraphs}.

            \begin{figure}
                \begin{center}
                    \includegraphics[width=\linewidth]{grasspaveloss.png}
                    \includegraphics[width=\linewidth]{grasspaveacc.png}
                \end{center}
                
                \caption{Loss and accuracy of grass and pavement detection network by training epoch.}
                \label{fig:grasspavegraphs}
            \end{figure}

            Because of the small size and binary output of the network, litle work was deemed neccesary to validate it's performance and there was no need for extensive eror analysis. A robot was manually driven around the track and the output of the network's predictions was monitured to ensure it was sensible before it was integrated with the state machine to trigger transitions between pavement and grass navigation states.

        \subsubsection{Crosswalk and Pedestrian Detection}
            When navigating over the first section of paved track, two crosswalks with pedestrians must be safely crossed without hitting any pedestrians. The robot must detect the presence of a crosswalk and transition to a `Crosswalk Wait` state in order to avoid hitting a pedestrian. Detection of both crosswalks and pedestrians is performed by methods within the `PedestrianDetector' class. First, crosswalks were detected using simple HSV colour thresholding. [figure here]. A crosswalk was considered 'detected' when 1500 total pixels matching the range of colours associated with the red crosswalk stop line were detected in the bottom 100 rows of the image. Although 1500 pixels is less than 2 full horizontal rows - much less than the size of the detection area - the need for a large area was the result of the relative high velocity which meant that the robot may only receive one or two frames to detect the crosswalk, so the larger area was chosen as a compromise between guaranteeing detection of the stop line and constantly stopping at around the same location. It also provides detection capability if the line is approached at an angle. This hedges against a possible 10 point loss from hitting a pedestrian of other parts of the navigation system fail.

            Once the crosswalk was detected and the robot was in the `Crosswalk Wait' state, we must detect pedestrians to determine when it is safe to cross. Pedestrians were detected using HSV thresholding to detect their blue jeans, similarly to the crosswalk detection. [figure here]. However, since the location (not just boolean presence) of the pedestrians was needed, erosion and dilation were used to merge the thresholded areas together into fewer, larger `blobs'. The algorithm then finds the contours of each `blob' and selects the largest as the most likely choice to be the pedestrian. Each frame, A bounding box is drawn around it, and its position is found.

            As a result of the repeatability of our pavement navigation algorithms, the edges of the crosswalk were able to be hard-coded into the pedestrian detector. The state machine tracks the position of the detector, and waits for it to be found in the crossawlk and then to exit the crosswalk. At this point, we can gurantee the maximum amount of time that the pedestrian will not be in the crosswalk, adnd the state machine is then able to transition to a `Crosswalk Traverse' sprint across the crosswalk before returning to the normal pavement navigation state. 
            
        \subsubsection{Junction and Truck Detection}
            In order to transition to the inner loop of track, the robot must turn through the junction to the inner loop, stop within the junction, wait for the truck to pass safely, and then proceed into the junction. This is accomplished through state transitions using methods with the `JunctionDetector' class.

            When the vehicle exits the grass segment, the state machine switches to the `Pave Navigate Left' mode, tracking the left side of the track rather than the right side as it does for other pavement navigation states. After three seconds of this, the state machine begins attempting to go through a procedure to see if has reached the junction with the inner track by monitoring the width of the pavement in front of it. The pavement width is determined by HSV thresholding to the colour of the pavement and counting the distance between the right- and leftmost valid pixels at a horizontal line 150 pixels up from the bottom of the frame. The algorithm used to evaluate if we are in the junction is shown in figure ~\ref{fig:junctionalg}, which works by exploiting the charachteristic pattern of the pavement appearing to widen, narrow, and then widen again as we turn into the junction [figure here!]. The state machine then transitions to a stopped `Junction Wait' state. 

            \begin{figure}
                \includegraphics[width=\linewidth]{junctiondiagram.png}
                \caption{Flowchart showing junction detection algorithm based on pavement width.}
                \label{fig:junctionalg}
            \end{figure}

            In the `Junction wait' state, the state machine waits to detect the truck in the appropriate position to safely start the inner loop navigation. Truck detection is based on a very similar methodology as pedestrian pant detection, using HSV colour thresholding followed by erosion, dilation, and contour finding. The state machine will proceed to the next state (moving into the inner loop) once the truck reaches a position in the frame of $x < 400$ pixels and $y<700 $ pixels, a zone that was experimentally determined safe. These values were able to be hard-coded because the position of the robot when waiting at the junction remained extremely consistent throughout runs. 


            
    \subsection{Navigation Algorithms}

        During the active navigation states, the navigation node processes each frame from the camera and uses it to determine a steering angle to publish based on an algorithm tailored for the state it is in. The two main types of active navigation nodes, for paved surfaces and grass surfaces employed relatively similar processes to determine steering angles by attempting to find the edge of the track and hold it at a constant bearing using a proportional controller to determine angular control and holding a constant speed. The main difference between the two was the method used to find the edge of the track.

        \subsubsection{Pavement Navigation}
            
            During pavement navigation, the edge of the track is found through an image processing pipeline to output a binary image with the edge of the track as white pixels and the rest of the image as black pixels. This binary image is then processed to find the center of the track, which is used to determine the steering angle. The image processing pipeline is shown in Figure~\ref{fig:pavepipeline}. The HSV threshold, greyscale, and binary threshold steps convert our frame into a single binary image that we are able to run edge detection and line finding on, while the dilation and erosion steps remove `holes' in the pavement caused by the environment's 1~m grid lines. Sobel edge detection is used to find the edges of the pavement, and then we crop the image to only include the pavement area of interest, ignoring the sky and trees. 
            
            A Hough transform is used to find the lines that best fit the edges, and these lines are then drawn onto a new, blank image which we can then convert to binary and use a simple weighted average to find the centroid of the edge, restricting ourselves to a zone between a certain number of pixels from the bottom of the image - to avoid looking too far into the future or focusing too much on lines immediately close to robot - and to either the left or right side of the image depending on if we are navigating based on the left or right side of the track. 

            We navigate based on the right side in almost all pavement areas, as it does not have any junctions interrupting the lines, but on the left side of the track in the `Pave Navigate Left' state, as we want to follow the left line into the junction in order to transition to the inner loop. 

            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.80\linewidth]{pavepipeline.png}
                \end{center}
                \caption{Pavement edge detection image processing pipeline.}
                \label{fig:pavepipeline}
            \end{figure}

        \subsubsection{Grass Navigation}

            Grass navigation was accomplished with a similar, but more simple pipeline than was used in the pavement navigation, shown in Figure~\ref{fig:grasspipeline}. However, instead of attempting to create lines that fit the edges of the track, we instead use thresholding and other classical techniques to simply isolate the pixels that match the colour of the lines, and then find the average position of the right line. A key step in being able to do this was to pass all frames through a brightening step that brings all images up to a standard contrast, as frames from the camera vary in contrast over different areas of the grassy section. Sobel edge detection is also used as a final step to reduce the impact of any large blocks of pixels in the final image other than the line, as they will only contribute an outline's worth of pixels, rather than the entire block. 

            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.3\linewidth]{grasspipeline.png}
                \end{center}
                \caption{Grass edge detection image processing pipeline}
                \label{fig:grasspipeline}
            \end{figure}

    \subsection{Performance}
        Through testing and iteration, we found our navigation system to be reliable once tuned, but not necessarily robust to modifications. For instance, we were able to run hundreds of laps on the track without any failures, but would not be able to change the speed without significant trial-and-error based retuning of multiple components and values. Additionally, we found that the navigation system was sensitive to the battery status of the computer the simulation was running on, and would routinely fail if the computer was not charged. We also encountered reliability challenges as we increased the speed of the robot. On the pavement sections, we were not able to reliably exceed 0.5~m/s, and on the grass sections, we were not able to reliably exceed 0.3~m/s without the robot losing the line and failing. We believe this to be a result of amount of track that the robot would cover between frames - the design of this navigation system, since it relies only on the current frame, is unable to use data about its previous navigation inputs or outputs to determine current outputs, and thus is unable to compensate for the robot's momentum. While these speed limits bounded the performance and lap time of the robot, we still found it extremely reliable within the limits of its maximum speed.


\section{License Plate Detection}

\section{User Interface}

\section{Conclusion}

\end{document}